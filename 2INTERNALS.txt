ID3



import pandas as pd
df = pd.read_csv('PlayTennis.csv')
print("\n Input Data Set is:\n", df)

t=df.keys()[-1]
print('Target Attribute is: ', t)
attribute_names = list(df.keys())
attribute_names.remove(t) 
print('Predicting Attributes: ', attribute_names)

import math
def entropy(probs):  
    return sum( [-prob*math.log(prob, 2) for prob in probs])

def entropy_of_list(ls,value):  
    from collections import Counter
    cnt = Counter(x for x in ls)
    print('Target attribute class count(Yes/No)=',dict(cnt))
    total_instances = len(ls)  
    print("Total no of instances/records associated with {0} is: {1}".format(value,total_instances ))
    probs = [x / total_instances for x in cnt.values()]  
    print("Probability of Class {0} is: {1:.4f}".format(min(cnt),min(probs)))
    print("Probability of Class {0} is: {1:.4f}".format(max(cnt),max(probs)))
    return entropy(probs) 

def information_gain(df, split_attribute, target_attribute,battr):
    print("\n\n-----Information Gain Calculation of ",split_attribute, " --------") 
    df_split = df.groupby(split_attribute)
    glist=[]
    for gname,group in df_split:
        print('Grouped Attribute Values \n',group)
        glist.append(gname) 
    
    glist.reverse()
    nobs = len(df.index) * 1.0   
    df_agg1=df_split.agg({target_attribute:lambda x:entropy_of_list(x, glist.pop())})
    df_agg2=df_split.agg({target_attribute :lambda x:len(x)/nobs})
    
    df_agg1.columns=['Entropy']
    df_agg2.columns=['Proportion']   

    new_entropy = sum( df_agg1['Entropy'] * df_agg2['Proportion'])
    if battr !='S':
        old_entropy = entropy_of_list(df[target_attribute],'S-'+df.iloc[0][df.columns.get_loc(battr)])
    else:
        old_entropy = entropy_of_list(df[target_attribute],battr)
    return old_entropy - new_entropy



def id3(df,target_attribute,attribute_names,default_class=None,default_attr='S'):
    
    from collections import Counter
    cnt = Counter(x for x in df[target_attribute])
    if len(cnt)==1:
        return next(iter(cnt))
    elif df.empty or (not attribute_names):
        return default_class
    else:
        default_class=max(cnt.keys())
        gainz=[]
        for attr in attribute_names:
            ig=information_gain(df,attr,target_attribute,default_attr)
            gainz.append(ig)
            print('Information gain of',attr,'is:',ig)
            
        index_of_max=gainz.index(max(gainz))
        best_attr=attribute_names[index_of_max]
        print("\nAttribute with the maximum gain is:",best_attr)
            
        tree={best_attr:{}}
        remaining_attribute_names=[i for i in attribute_names if i!=best_attr]
            
        for attr_val,data_subset in df.groupby (best_attr):
            subtree=id3(data_subset,target_attribute,remaining_attribute_names,default_class,best_attr)
            tree[best_attr][attr_val]=subtree
        return tree

from pprint import pprint
tree = id3(df,t,attribute_names)
print("\nthe Resultant Decision Tree is:")
pprint(tree)

def classify(instance,tree,default=None):
    attribute=next(iter(tree))
    if instance[attribute] in tree[attribute].keys():
        result = tree[attribute][instance[attribute]]
        if isinstance (result,dict):
            return classify(instance,result)
        else:
            return result
    else:
        return default

df_new = pd.read_csv('PlayTennisTest.csv')
df_new['predicted'] = df_new.apply(classify,axis=1,args=(tree,'?'))
print(df_new)


NAVYE BASE



import numpy as np
import math
import csv

def read_data(filename):
    with open(filename, 'r') as csvfile:
        datareader = csv.reader(csvfile)
        metadata = next(datareader)
        traindata=[]
        for row in datareader:
            traindata.append(row)
    return (metadata, traindata)


def splitDataset(dataset, splitRatio): 
    trainSize = int(len(dataset) * splitRatio)
    trainSet = []
    testset = list(dataset)
    i=0
    while len(trainSet) < trainSize:
        trainSet.append(testset.pop(i))
    return [trainSet, testset]

def classify(data,test):
    total_size = data.shape[0]
    print("training data size=",total_size)
    print("test data size=",test.shape[0])
    target=np.unique(data[:,-1])
    count = np.zeros((target.shape[0]), dtype=np.int32)
    prob = np.zeros((target.shape[0]), dtype=np.float32)
    
    print("target count probability")
    
    for y in range(target.shape[0]):
        for x in range(data.shape[0]):
            if data[x,data.shape[1]-1] == target[y]:
                count[y] += 1
        prob[y]=count[y]/total_size # comptes the probability of target
        print(target[y],"\t",count[y],"\t",prob[y])
        
    prob0 = np.zeros((test.shape[1]-1), dtype=np.float32)
    prob1 = np.zeros((test.shape[1]-1), dtype=np.float32)
    accuracy=0
    print("Instance prediction target")
    for t in range(test.shape[0]):
        for k in range(test.shape[1]-1): 
            count1=count0=0
            for j in range(data.shape[0]):
                if test[t,k]== data[j,k] and data[j,data.shape[1]-1]== target[0]:
                    count0+=1
                elif test[t,k]== data[j,k] and data[j,data.shape[1]-1]== target[1]:
                    count1+=1
            prob0[k]= count0/count[0] 
            prob1[k]= count1/count[1]
    
        probno=prob[0]
        probyes=prob[1]
        for i in range(test.shape[1]-1):
            probno=probno*prob0[i]
            probyes=probyes*prob1[i]
    
        if probno>probyes: 
            predict='no'
        else:
            predict='yes'
        print(t+1,"\t",predict,"\t ",test[t,test.shape[1]-1])
    
        if predict== test[t,test.shape[1]-1]:
            accuracy+=1
        final_accuracy=(accuracy/test.shape[0])*100
        print("accuracy",final_accuracy,"%")
    return

metadata, traindata = read_data("PlayTennis.csv")
splitRatio = 0.6
trainingset, testset = splitDataset(traindata, splitRatio)
training=np.array(trainingset)
testing=np.array(testset)
print("------------------Training Data ------------------ ")
print(trainingset)
print("-------------------Test Data ------------------ ")
print(testset)

classify(training,testing)



ANN



import numpy as np
X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)
y = np.array(([92], [86], [89]), dtype=float)
X = X/np.amax(X,axis=0)
y = y/100


def sigmoid (x):
    return 1/(1 + np.exp(-x))

def derivatives_sigmoid(x):
    return x * (1 - x)


epoch=7000 
learning_rate=0.1 
inputlayer_neurons = 2 
hiddenlayer_neurons = 3 
output_neurons = 1 

wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons)) 
bh=np.random.uniform(size=(1,hiddenlayer_neurons))
wo=np.random.uniform(size=(hiddenlayer_neurons,output_neurons)) 
bo=np.random.uniform(size=(1,output_neurons))

for i in range(epoch):

    net_h=np.dot(X,wh) + bh 
    sigma_h= sigmoid(net_h) 
    net_o= np.dot(sigma_h,wo)+ bo 
    output = sigmoid(net_o)
    
    deltaK = (y-output)* derivatives_sigmoid(output) 
    deltaH =  deltaK.dot(wo.T) * derivatives_sigmoid(sigma_h) 
    wo = wo + sigma_h.T.dot(deltaK) *learning_rate 
    wh = wh + X.T.dot(deltaH) *learning_rate 

print("Input: \n" + str(X))
print("Actual Output: \n" + str(y))
print("Predicted Output: \n" ,output)






